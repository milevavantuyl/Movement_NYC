{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6884c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "import pyspark.sql.types as T\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37050f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/uber/uber-trip-data/uber-raw-data-jun14.csv',\n",
       " '../data/uber/uber-trip-data/uber-raw-data-may14.csv',\n",
       " '../data/uber/uber-trip-data/taxi-zone-lookup.csv',\n",
       " '../data/uber/uber-trip-data/uber-raw-data-jul14.csv',\n",
       " '../data/uber/uber-trip-data/uber-raw-data-sep14.csv',\n",
       " '../data/uber/uber-trip-data/uber-raw-data-apr14.csv',\n",
       " '../data/uber/uber-trip-data/uber-raw-data-aug14.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"../data/uber/uber-trip-data/\"\n",
    "uber_files = glob.glob(data_dir + '*.csv') \n",
    "uber_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7a5eb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_files.remove('../data/uber/uber-trip-data/taxi-zone-lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c63d77ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/uber/uber-trip-data/uber-raw-data-jun14.csv',\n",
       " '../data/uber/uber-trip-data/uber-raw-data-may14.csv',\n",
       " '../data/uber/uber-trip-data/uber-raw-data-jul14.csv',\n",
       " '../data/uber/uber-trip-data/uber-raw-data-sep14.csv',\n",
       " '../data/uber/uber-trip-data/uber-raw-data-apr14.csv',\n",
       " '../data/uber/uber-trip-data/uber-raw-data-aug14.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60802030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/07 14:35:17 WARN Utils: Your hostname, DSGPU05 resolves to a loopback address: 127.0.1.1; using 10.10.11.64 instead (on interface eno1)\n",
      "22/05/07 14:35:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/07 14:35:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Word Count\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "867ad3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(uber_files, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aef289f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+--------+------+\n",
      "|       Date/Time|    Lat|     Lon|  Base|\n",
      "+----------------+-------+--------+------+\n",
      "|9/1/2014 0:01:00|40.2201|-74.0021|B02512|\n",
      "|9/1/2014 0:01:00|  40.75|-74.0027|B02512|\n",
      "|9/1/2014 0:03:00|40.7559|-73.9864|B02512|\n",
      "|9/1/2014 0:06:00| 40.745|-73.9889|B02512|\n",
      "|9/1/2014 0:11:00|40.8145|-73.9444|B02512|\n",
      "|9/1/2014 0:12:00|40.6735|-73.9918|B02512|\n",
      "|9/1/2014 0:15:00|40.7471|-73.6472|B02512|\n",
      "|9/1/2014 0:16:00|40.6613|-74.2691|B02512|\n",
      "|9/1/2014 0:32:00|40.3745|-73.9999|B02512|\n",
      "|9/1/2014 0:33:00|40.7633|-73.9773|B02512|\n",
      "|9/1/2014 0:33:00|40.7467|-73.6131|B02512|\n",
      "|9/1/2014 0:37:00|40.8105|  -73.96|B02512|\n",
      "|9/1/2014 0:38:00| 40.679|-74.0111|B02512|\n",
      "|9/1/2014 0:39:00|40.4023|-73.9839|B02512|\n",
      "|9/1/2014 0:48:00|40.7378|-74.0395|B02512|\n",
      "|9/1/2014 0:48:00|40.7214|-73.9884|B02512|\n",
      "|9/1/2014 0:49:00|40.8646|-73.9081|B02512|\n",
      "|9/1/2014 1:08:00|40.7398|-74.0061|B02512|\n",
      "|9/1/2014 1:17:00|40.6793|-74.0116|B02512|\n",
      "|9/1/2014 1:19:00|40.7328|-73.9875|B02512|\n",
      "+----------------+-------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c784511",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "df = df.withColumn(\"Date/Time\", F.to_date(F.to_timestamp(F.col(\"Date/Time\"), \"MM/d/yyyy H:mm:ss\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3ec0c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------+------+\n",
      "| Date/Time|    Lat|     Lon|  Base|\n",
      "+----------+-------+--------+------+\n",
      "|2014-09-01|40.2201|-74.0021|B02512|\n",
      "|2014-09-01|  40.75|-74.0027|B02512|\n",
      "|2014-09-01|40.7559|-73.9864|B02512|\n",
      "|2014-09-01| 40.745|-73.9889|B02512|\n",
      "|2014-09-01|40.8145|-73.9444|B02512|\n",
      "|2014-09-01|40.6735|-73.9918|B02512|\n",
      "|2014-09-01|40.7471|-73.6472|B02512|\n",
      "|2014-09-01|40.6613|-74.2691|B02512|\n",
      "|2014-09-01|40.3745|-73.9999|B02512|\n",
      "|2014-09-01|40.7633|-73.9773|B02512|\n",
      "|2014-09-01|40.7467|-73.6131|B02512|\n",
      "|2014-09-01|40.8105|  -73.96|B02512|\n",
      "|2014-09-01| 40.679|-74.0111|B02512|\n",
      "|2014-09-01|40.4023|-73.9839|B02512|\n",
      "|2014-09-01|40.7378|-74.0395|B02512|\n",
      "|2014-09-01|40.7214|-73.9884|B02512|\n",
      "|2014-09-01|40.8646|-73.9081|B02512|\n",
      "|2014-09-01|40.7398|-74.0061|B02512|\n",
      "|2014-09-01|40.6793|-74.0116|B02512|\n",
      "|2014-09-01|40.7328|-73.9875|B02512|\n",
      "+----------+-------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf6cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
